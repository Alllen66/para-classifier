import os
import json
import uuid
import threading
from datetime import datetime
from flask import Blueprint, request, jsonify
from werkzeug.utils import secure_filename
from volcenginesdkarkruntime import Ark

classifier_bp = Blueprint('classifier', __name__)

# å­˜å‚¨åˆ†æä»»åŠ¡çš„çŠ¶æ€
analysis_tasks = {}

# è±†åŒ…APIé…ç½®
DEFAULT_MODEL = "doubao-seed-1-6-thinking-250615"

def get_file_info(file_path):
    """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
    try:
        stat = os.stat(file_path)
        return {
            'name': os.path.basename(file_path),
            'size': stat.st_size,
            'extension': os.path.splitext(file_path)[1].lower(),
            'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'path': file_path,
            'original_directory': os.path.basename(os.path.dirname(file_path))
        }
    except Exception as e:
        return None

def read_file_content(file_path, max_chars=500):
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä»…æ–‡æœ¬æ–‡ä»¶ï¼‰"""
    text_extensions = ['.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv', '.doc', '.docx']
    
    try:
        ext = os.path.splitext(file_path)[1].lower()
        if ext not in text_extensions:
            return "éæ–‡æœ¬æ–‡ä»¶ï¼Œæ— æ³•é¢„è§ˆ"
            
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(max_chars)
            return content
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}"

def scan_target_directory_structure(target_path):
    """æ‰«æç›®æ ‡æ–‡ä»¶å¤¹çš„ç°æœ‰ç›®å½•ç»“æ„"""
    structure = {}
    try:
        if not os.path.exists(target_path):
            return {}
        
        for item in os.listdir(target_path):
            item_path = os.path.join(target_path, item)
            if os.path.isdir(item_path):
                # è®°å½•ä¸€çº§ç›®å½•
                subdirs = []
                try:
                    for subitem in os.listdir(item_path):
                        subitem_path = os.path.join(item_path, subitem)
                        if os.path.isdir(subitem_path):
                            subdirs.append(subitem)
                except PermissionError:
                    continue
                structure[item] = subdirs
        
        # è¯¦ç»†æ‰“å°ç›®å½•ç»“æ„
        for dir_name, subdirs in structure.items():
            if subdirs:
                for subdir in subdirs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­ç›®å½•
                if len(subdirs) > 5:
            else:
        return structure
    except Exception as e:
        print(f"âŒ æ‰«æç›®æ ‡ç›®å½•ç»“æ„å¤±è´¥: {e}")
        return {}

def validate_classification_plan(classification_plan, existing_structure, target_base_path):
    """éªŒè¯AIè¿”å›çš„åˆ†ç±»æ–¹æ¡ˆæ˜¯å¦ä½¿ç”¨äº†ç°æœ‰ç›®å½•ç»“æ„"""
    errors = []
    warnings = []
    
    # æ£€æŸ¥å¿…è¦çš„é”®
    if 'mapping_table' not in classification_plan:
        errors.append("ç¼ºå°‘ mapping_table é”®")
    if 'directory_structure' not in classification_plan:
        errors.append("ç¼ºå°‘ directory_structure é”®")
    
    if errors:
        return {'valid': False, 'errors': errors, 'warnings': warnings}
    
    existing_dirs = set(existing_structure.keys())
    mapping_table = classification_plan.get('mapping_table', [])
    
    
    # éªŒè¯æ¯ä¸ªæ–‡ä»¶çš„æ–°è·¯å¾„
    for i, item in enumerate(mapping_table):
        if 'new_directory' not in item:
            errors.append(f"æ–‡ä»¶ {item.get('filename', 'unknown')} ç¼ºå°‘ new_directory å­—æ®µ")
            continue
            
        new_path = item['new_directory']
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥ç›®æ ‡è·¯å¾„å¼€å¤´
        if not new_path.startswith(target_base_path):
            errors.append(f"æ–‡ä»¶ {item.get('filename')} çš„è·¯å¾„ä¸åœ¨ç›®æ ‡ç›®å½•ä¸‹: {new_path}")
            continue
        
        # æå–ä¸€çº§ç›®å½•
        relative_path = new_path[len(target_base_path):].lstrip('/')
        if not relative_path:
            errors.append(f"æ–‡ä»¶ {item.get('filename')} çš„è·¯å¾„æ— æ•ˆ: {new_path}")
            continue
            
        first_dir = relative_path.split('/')[0]
        
        # æ£€æŸ¥ä¸€çº§ç›®å½•æ˜¯å¦åœ¨ç°æœ‰ç›®å½•ä¸­
        if first_dir not in existing_dirs:
            errors.append(f"æ–‡ä»¶ {item.get('filename')} ä½¿ç”¨äº†ä¸å­˜åœ¨çš„ä¸€çº§ç›®å½•: {first_dir}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ­§ä¹‰æ ‡è®°
        if "(æ­§ä¹‰ï¼Œéœ€è®¨è®º)" in new_path:
            warnings.append(f"æ–‡ä»¶ {item.get('filename')} è¢«æ ‡è®°ä¸ºæ­§ä¹‰æ–‡ä»¶")
        
        # æ£€æŸ¥é¡¹ç›®æ–‡ä»¶æ˜¯å¦è¢«é”™è¯¯å½’æ¡£
        filename = item.get('filename', '').lower()
        if first_dir in existing_dirs and any(archive_keyword in first_dir.lower() for archive_keyword in ['archive', 'å½’æ¡£', '04-']):
            if any(project_keyword in filename for project_keyword in ['é¡¹ç›®', 'project', 'å¤ç›˜', 'æ€»ç»“', 'ç»éªŒ']):
                warnings.append(f"æ–‡ä»¶ {item.get('filename')} åŒ…å«é¡¹ç›®ç›¸å…³å†…å®¹ä½†è¢«æ”¾å…¥å½’æ¡£ç›®å½•ï¼Œè¯·æ£€æŸ¥æ˜¯å¦åº”è¯¥æ”¾å…¥Projects")
        
        # æ£€æŸ¥èµ„æºæ–‡ä»¶æ˜¯å¦æ”¾é”™ä½ç½®
        if filename.endswith(('.md', '.pdf', '.doc', '.docx')) and any(project_keyword in filename for project_keyword in ['é¡¹ç›®', 'project']):
            if first_dir in existing_dirs and any(resource_keyword in first_dir.lower() for resource_keyword in ['resource', 'èµ„æº', '03-']):
                warnings.append(f"æ–‡ä»¶ {item.get('filename')} ä¼¼ä¹æ˜¯é¡¹ç›®æ–‡ä»¶ä½†è¢«æ”¾å…¥èµ„æºç›®å½•ï¼Œè¯·æ£€æŸ¥åˆ†ç±»")
    
    # éªŒè¯ç›®å½•ç»“æ„
    directory_structure = classification_plan.get('directory_structure', {})
    for top_dir in directory_structure.keys():
        if top_dir not in existing_dirs:
            errors.append(f"ç›®å½•ç»“æ„ä¸­åŒ…å«ä¸å­˜åœ¨çš„ä¸€çº§ç›®å½•: {top_dir}")
    
    is_valid = len(errors) == 0
    print(f"ğŸ“‹ éªŒè¯ç»“æœ: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}, é”™è¯¯æ•°: {len(errors)}, è­¦å‘Šæ•°: {len(warnings)}")
    
    return {
        'valid': is_valid,
        'errors': errors,
        'warnings': warnings
    }

def fix_classification_paths(classification_plan, existing_structure, target_base_path):
    """å°è¯•ä¿®å¤åˆ†ç±»æ–¹æ¡ˆä¸­çš„è·¯å¾„é—®é¢˜"""
    if not existing_structure:
        return None
    
    existing_dirs = list(existing_structure.keys())
    mapping_table = classification_plan.get('mapping_table', [])
    
    
    # PARAç›®å½•æ˜ å°„è§„åˆ™
    para_mapping = {
        'project': [d for d in existing_dirs if any(keyword in d.lower() for keyword in ['project', 'é¡¹ç›®', '01-', '10-'])],
        'area': [d for d in existing_dirs if any(keyword in d.lower() for keyword in ['area', 'é¢†åŸŸ', '02-', '20-'])],
        'resource': [d for d in existing_dirs if any(keyword in d.lower() for keyword in ['resource', 'èµ„æº', '03-', '30-'])],
        'archive': [d for d in existing_dirs if any(keyword in d.lower() for keyword in ['archive', 'å½’æ¡£', '04-', '40-'])]
    }
    
    fixed_count = 0
    
    for item in mapping_table:
        new_path = item.get('new_directory', '')
        if not new_path:
            continue
            
        # æå–å½“å‰ä½¿ç”¨çš„ä¸€çº§ç›®å½•
        relative_path = new_path[len(target_base_path):].lstrip('/')
        if not relative_path:
            continue
            
        current_first_dir = relative_path.split('/')[0]
        
        # å¦‚æœä¸€çº§ç›®å½•ä¸åœ¨ç°æœ‰ç›®å½•ä¸­ï¼Œå°è¯•ä¿®å¤
        if current_first_dir not in existing_dirs:
            
            # å°è¯•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            best_match = None
            
            # æ™ºèƒ½åˆ¤æ–­æ–‡ä»¶åº”è¯¥æ”¾åœ¨å“ªä¸ªåˆ†ç±»
            filename = item.get('filename', '').lower()
            original_lower = current_first_dir.lower()
            
            # ä¼˜å…ˆæ ¹æ®æ–‡ä»¶åå†…å®¹åˆ¤æ–­
            if any(keyword in filename for keyword in ['é¡¹ç›®', 'project', 'å¤ç›˜', 'æ€»ç»“', 'ç»éªŒ']):
                best_match = para_mapping['project'][0] if para_mapping['project'] else existing_dirs[0]
            elif any(keyword in filename for keyword in ['æ•™ç¨‹', 'tutorial', 'æŒ‡å—', 'guide', 'æ¨¡æ¿', 'template']):
                best_match = para_mapping['resource'][0] if para_mapping['resource'] else existing_dirs[0]
            # ç„¶åæ ¹æ®åŸç›®å½•åç§°çŒœæµ‹PARAåˆ†ç±»
            elif any(keyword in original_lower for keyword in ['project', 'é¡¹ç›®']):
                best_match = para_mapping['project'][0] if para_mapping['project'] else existing_dirs[0]
            elif any(keyword in original_lower for keyword in ['area', 'é¢†åŸŸ']):
                best_match = para_mapping['area'][0] if para_mapping['area'] else existing_dirs[0]
            elif any(keyword in original_lower for keyword in ['resource', 'èµ„æº']):
                best_match = para_mapping['resource'][0] if para_mapping['resource'] else existing_dirs[0]
            elif any(keyword in original_lower for keyword in ['archive', 'å½’æ¡£']):
                best_match = para_mapping['archive'][0] if para_mapping['archive'] else existing_dirs[0]
            else:
                # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªç°æœ‰ç›®å½•
                best_match = existing_dirs[0]
            
            if best_match:
                # æ›¿æ¢ä¸€çº§ç›®å½•
                path_parts = relative_path.split('/')
                path_parts[0] = best_match
                new_relative_path = '/'.join(path_parts)
                item['new_directory'] = os.path.join(target_base_path, new_relative_path).replace('\\', '/')
                fixed_count += 1
    
    if fixed_count > 0:
        return classification_plan
    else:
        print("âŒ æ²¡æœ‰è·¯å¾„éœ€è¦ä¿®å¤æˆ–ä¿®å¤å¤±è´¥")
        return None

def generate_classification_plan_with_ai(files_info, target_base_path, api_key):
    """(æ–°) ä½¿ç”¨ AI ä¸ºæ‰€æœ‰æ–‡ä»¶ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„åˆ†ç±»æ–¹æ¡ˆ"""
    
    
    # æ‰«æç›®æ ‡æ–‡ä»¶å¤¹çš„ç°æœ‰ç»“æ„
    existing_structure = scan_target_directory_structure(target_base_path)
    
    # 4. ä¼˜åŒ–è¾¹ç•Œå¤„ç†ï¼šå¤„ç†ç©ºç›®å½•çš„æƒ…å†µ
    if not existing_structure:
        # åˆ›å»ºæ ‡å‡†PARAç›®å½•ç»“æ„
        standard_para_dirs = ['01-Projects', '02-Areas', '03-Resources', '04-Archives']
        for dir_name in standard_para_dirs:
            dir_path = os.path.join(target_base_path, dir_name)
            os.makedirs(dir_path, exist_ok=True)
        
        # é‡æ–°æ‰«æç»“æ„
        existing_structure = scan_target_directory_structure(target_base_path)
    
    # æ„å»ºæ–‡ä»¶æ‘˜è¦ä¿¡æ¯
    files_summary = []
    for file_info in files_info:
        summary = {
            'filename': file_info['name'],
            'original_directory': file_info['original_directory'],
            'content_preview': file_info.get('content_preview', '')[:200]
        }
        files_summary.append(summary)
    

    # 2. å¼ºåŒ–AIæç¤ºè¯ï¼šæ›´æ˜ç¡®è¦æ±‚ä½¿ç”¨ç°æœ‰ç›®å½•ï¼Œæ·»åŠ æ›´å¤šç¤ºä¾‹
    existing_dirs_list = list(existing_structure.keys())
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ä¸ªäººçŸ¥è¯†ç®¡ç†ä¸“å®¶ï¼Œç²¾é€š PARA æ–¹æ³•ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªå®Œæ•´ã€æ™ºèƒ½ä¸”å¯æ“ä½œçš„æ–‡ä»¶æ•´ç†æ–¹æ¡ˆã€‚

**âš ï¸ é‡è¦çº¦æŸæ¡ä»¶ - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š**
1. ä½ å¿…é¡»ä¸”åªèƒ½ä½¿ç”¨ä»¥ä¸‹ç°æœ‰ç›®å½•ä½œä¸ºä¸€çº§åˆ†ç±»ï¼š{existing_dirs_list}
2. ç»å¯¹ä¸å¯ä»¥åˆ›å»ºæ–°çš„ä¸€çº§ç›®å½•
3. æ‰€æœ‰æ–‡ä»¶çš„æ–°è·¯å¾„éƒ½å¿…é¡»ä»¥è¿™äº›ç°æœ‰ç›®å½•ä¹‹ä¸€å¼€å¤´
4. å¦‚æœä½ ä¸ç¡®å®šå¦‚ä½•åˆ†ç±»æŸä¸ªæ–‡ä»¶ï¼Œè¯·åœ¨è·¯å¾„ä¸­æ ‡æ³¨"(æ­§ä¹‰ï¼Œéœ€è®¨è®º)"

**PARA æ–¹æ³•æ ¸å¿ƒåŸåˆ™:**
- **Projects (é¡¹ç›®):** æ­£åœ¨è¿›è¡Œçš„ã€æœ‰æ˜ç¡®ç›®æ ‡å’Œäº¤ä»˜æ—¥æœŸçš„ä»»åŠ¡ã€‚åŒ…æ‹¬é¡¹ç›®è®¡åˆ’ã€è¿›åº¦è®°å½•ã€å¤ç›˜æ€»ç»“ã€é¡¹ç›®èµ„æ–™ç­‰ã€‚åªè¦é¡¹ç›®è¿˜åœ¨æ¨è¿›æˆ–æœ‰å‚è€ƒä»·å€¼ï¼Œéƒ½åº”å½’å…¥æ­¤ç±»ã€‚
- **Areas (é¢†åŸŸ):** éœ€è¦æŒç»­å…³æ³¨å’Œç»´æŠ¤çš„ä¸ªäººæˆ–å·¥ä½œè´£ä»»åŒºï¼Œæ²¡æœ‰æ˜ç¡®çš„ç»“æŸæ—¥æœŸã€‚å¦‚ä¸ªäººæˆé•¿ã€å¥åº·ç®¡ç†ã€è´¢åŠ¡è§„åˆ’ç­‰ã€‚
- **Resources (èµ„æº):** å¯¹å¤šä¸ªé¡¹ç›®æˆ–é¢†åŸŸéƒ½æœ‰ç”¨çš„å‚è€ƒèµ„æ–™å’Œä¿¡æ¯ã€‚å¦‚æŠ€æœ¯æ–‡æ¡£ã€æ¨¡æ¿ã€å·¥å…·ä½¿ç”¨æŒ‡å—ã€å­¦ä¹ èµ„æ–™ç­‰ã€‚
- **Archives (å½’æ¡£):** æ˜ç¡®å·²ç»å®Œå…¨ç»“æŸã€ä¸å†æœ‰ä»»ä½•ä»·å€¼æˆ–å‚è€ƒæ„ä¹‰çš„å†…å®¹ã€‚æ³¨æ„ï¼šé¡¹ç›®å¤ç›˜ã€æ€»ç»“ã€ç»éªŒè®°å½•é€šå¸¸ä»æœ‰å‚è€ƒä»·å€¼ï¼Œåº”æ”¾åœ¨Projectsä¸­ã€‚

**â—é‡è¦åˆ†ç±»æŒ‡å¯¼åŸåˆ™:**
1. **é¡¹ç›®å¤ç›˜â‰ å½’æ¡£**: "é¡¹ç›®å¤ç›˜.md"ã€"é¡¹ç›®æ€»ç»“.md"ã€"ç»éªŒåˆ†äº«.md" ç­‰æ–‡ä»¶é€šå¸¸åº”è¯¥æ”¾åœ¨Projectsä¸­ï¼Œå› ä¸ºå®ƒä»¬å¯¹åç»­é¡¹ç›®æœ‰å‚è€ƒä»·å€¼
2. **æ´»è·ƒåˆ¤æ–­æ ‡å‡†**: 
   - Projects: æ­£åœ¨è¿›è¡Œçš„é¡¹ç›® + æœ‰å‚è€ƒä»·å€¼çš„é¡¹ç›®èµ„æ–™(åŒ…æ‹¬å¤ç›˜ã€æ€»ç»“)
   - Archives: çœŸæ­£è¿‡æ—¶ã€æ— å‚è€ƒä»·å€¼ã€å®Œå…¨åºŸå¼ƒçš„å†…å®¹
3. **ç–‘æƒ‘æ—¶ä¼˜å…ˆProjects**: å½“ä¸ç¡®å®šæ˜¯å¦å½’æ¡£æ—¶ï¼Œä¼˜å…ˆæ”¾å…¥Projectsæˆ–ç›¸å…³Areas

**ç°æœ‰ç›®å½•ç»“æ„è¯¦æƒ…:**
```json
{json.dumps(existing_structure, ensure_ascii=False, indent=2)}
```

**æ™ºèƒ½åŒ¹é…è§„åˆ™ç¤ºä¾‹:**
- åŒ…å«"project"ã€"é¡¹ç›®"ã€æ•°å­—ç¼–å·(å¦‚"01-"ã€"10-")çš„ç›®å½•é€šå¸¸å¯¹åº”Projects
- åŒ…å«"area"ã€"é¢†åŸŸ"ã€"20-"çš„ç›®å½•é€šå¸¸å¯¹åº”Areas  
- åŒ…å«"resource"ã€"èµ„æº"ã€"30-"çš„ç›®å½•é€šå¸¸å¯¹åº”Resources
- åŒ…å«"archive"ã€"å½’æ¡£"ã€"40-"çš„ç›®å½•é€šå¸¸å¯¹åº”Archives

**æ–‡ä»¶åˆ†ç±»å…·ä½“ç¤ºä¾‹:**
ğŸ“ **æ”¾å…¥Projectsçš„æ–‡ä»¶:**
- "å‰¯ä¸šæ¢ç´¢-çŸ¥è¯†ä»˜è´¹é¡¹ç›®å¤ç›˜.md" â†’ 01-Projects/å‰¯ä¸šé¡¹ç›®/çŸ¥è¯†ä»˜è´¹/
- "ç”µå•†æ¨èç³»ç»Ÿä¼˜åŒ–é¡¹ç›®å¤ç›˜.md" â†’ 01-Projects/ç”µå•†é¡¹ç›®/æ¨èç³»ç»Ÿ/
- "é¡¹ç›®è®¡åˆ’.md"ã€"è¿›åº¦æŠ¥å‘Š.pdf" â†’ 01-Projects/å¯¹åº”é¡¹ç›®/
- "ç»éªŒæ€»ç»“.md"ã€"æ•™è®­è®°å½•.md" â†’ 01-Projects/å¯¹åº”é¡¹ç›®/

ğŸ“ **æ”¾å…¥Resourcesçš„æ–‡ä»¶:**
- "ç¼–ç¨‹æŠ€å·§.md"ã€"è®¾è®¡æ¨¡æ¿.psd" â†’ 03-Resources/æŠ€æœ¯èµ„æ–™/
- "è¡Œä¸šåˆ†ææŠ¥å‘Š.pdf" â†’ 03-Resources/è¡Œä¸šç ”ç©¶/

ğŸ“ **æ”¾å…¥Archivesçš„æ–‡ä»¶:**
- "2020å¹´åºŸå¼ƒçš„æƒ³æ³•.md" â†’ 04-Archives/å†å²è®°å½•/
- "è¿‡æ—¶çš„æŠ€æœ¯æ–‡æ¡£.pdf" â†’ 04-Archives/æŠ€æœ¯èµ„æ–™/

**æ–‡ä»¶åˆ—è¡¨:**
```json
{json.dumps(files_summary, ensure_ascii=False, indent=2)}
```

**ç›®æ ‡æ ¹ç›®å½•:** `{target_base_path}`

---

**ä»»åŠ¡ä¸€: ç”Ÿæˆ"æ–‡ä»¶å-åŸç›®å½•-æ–°ç›®å½•"æ˜ å°„è¡¨ (mapping_table)**

**ä¸¥æ ¼è¦æ±‚:**
- æ–°ç›®å½•è·¯å¾„æ ¼å¼ï¼š`{target_base_path}/[å¿…é¡»æ˜¯ç°æœ‰ä¸€çº§ç›®å½•]/[äºŒçº§åˆ†ç±»]/[ä¸‰çº§åˆ†ç±»å¯é€‰]/æ–‡ä»¶å`
- ä¸€çº§ç›®å½•å¿…é¡»ä»è¿™ä¸ªåˆ—è¡¨ä¸­é€‰æ‹©ï¼š{existing_dirs_list}
- ç¤ºä¾‹æ­£ç¡®è·¯å¾„ï¼š`{target_base_path}/{existing_dirs_list[0] if existing_dirs_list else "01-Projects"}/ç¼–ç¨‹å­¦ä¹ /PythonåŸºç¡€/example.py`
- é”™è¯¯è·¯å¾„ç¤ºä¾‹ï¼š`{target_base_path}/Projects/...` (å› ä¸ºProjectsä¸åœ¨ç°æœ‰ç›®å½•åˆ—è¡¨ä¸­)

**âš ï¸ å…³é”®åˆ†ç±»æ³¨æ„äº‹é¡¹:**
- åŒ…å«"é¡¹ç›®"ã€"å¤ç›˜"ã€"æ€»ç»“"ã€"ç»éªŒ"çš„æ–‡ä»¶ï¼Œä¼˜å…ˆè€ƒè™‘æ”¾å…¥Projectsç›®å½•
- åªæœ‰çœŸæ­£è¿‡æ—¶ã€æ— ä»·å€¼çš„å†…å®¹æ‰æ”¾å…¥Archives
- å½“æ–‡ä»¶ååŒ…å«å…·ä½“é¡¹ç›®åç§°æ—¶(å¦‚"ç”µå•†é¡¹ç›®"ã€"å‰¯ä¸šé¡¹ç›®")ï¼Œåº”æ”¾å…¥å¯¹åº”çš„Projectså­ç›®å½•
- æŠ€æœ¯æ–‡æ¡£ã€æ•™ç¨‹ã€æ¨¡æ¿ç­‰é€šç”¨èµ„æ–™æ”¾å…¥Resources

**ä»»åŠ¡äºŒ: ç”Ÿæˆæ–°ç›®å½•çš„å®Œæ•´ç»“æ„ (directory_structure)**
- é¡¶çº§é”®å¿…é¡»ä¸”åªèƒ½æ˜¯ç°æœ‰ç›®å½•ï¼š{existing_dirs_list}
- ä¸è¦åˆ›å»ºæ–°çš„é¡¶çº§ç›®å½•

**ä»»åŠ¡ä¸‰: ç”Ÿæˆè®¨è®ºç‚¹ (discussion_points)**
å¯¹äºæ— æ³•æ˜ç¡®åˆ†ç±»çš„æ–‡ä»¶ï¼Œæä¾›å»ºè®®ã€‚

**æœ€ç»ˆè¾“å‡ºæ ¼å¼:**
```json
{{
  "mapping_table": [
    {{
      "filename": "æ–‡ä»¶å.ext",
      "original_directory": "åŸç›®å½•",
      "new_directory": "{target_base_path}/[ç°æœ‰ç›®å½•]/å­åˆ†ç±»/æ–‡ä»¶å.ext"
    }}
  ],
  "directory_structure": {{
    "{existing_dirs_list[0] if existing_dirs_list else "01-Projects"}": {{}},
    "{existing_dirs_list[1] if len(existing_dirs_list) > 1 else "02-Areas"}": {{}}
  }},
  "discussion_points": []
}}
```
"""

    try:
        if not api_key:
            raise ValueError("API Key æœªæä¾›")


        # ä½¿ç”¨è±†åŒ…SDK
        client = Ark(api_key=api_key)
        
        completion = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=4000
        )
        
        ai_response = completion.choices[0].message.content.strip()
        
        # å°è¯•è§£æ AI è¿”å›çš„ JSON
        try:
            # æå– JSON éƒ¨åˆ†
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            if start_idx == -1 or end_idx == 0:
                raise json.JSONDecodeError("æ— æ³•åœ¨AIå“åº”ä¸­æ‰¾åˆ°JSONå¯¹è±¡", ai_response, 0)
            
            json_str = ai_response[start_idx:end_idx]
            classification_plan = json.loads(json_str)

            # 3. æ·»åŠ ç»“æœéªŒè¯ï¼šæ£€æŸ¥AIè¿”å›çš„è·¯å¾„æ˜¯å¦ä½¿ç”¨äº†ç°æœ‰ç›®å½•
            validation_result = validate_classification_plan(classification_plan, existing_structure, target_base_path)
            if validation_result['valid']:
                if validation_result['warnings']:
                return classification_plan
            else:
                print(f"âŒ åˆ†ç±»æ–¹æ¡ˆéªŒè¯å¤±è´¥: {validation_result['errors']}")
                fixed_plan = fix_classification_paths(classification_plan, existing_structure, target_base_path)
                if fixed_plan:
                    return fixed_plan
                else:
                    print("âŒ è·¯å¾„ä¿®å¤å¤±è´¥")
                    return None
        except json.JSONDecodeError as e:
            print(f"âŒ AI è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {ai_response[:500]}..., é”™è¯¯: {e}")
            return None
            
    except Exception as e:
        print(f"ğŸ’¥ AI åˆ†ç±»æ–¹æ¡ˆç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        print(f"ğŸ“‹ å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        return None

def generate_classification_plan_with_ai_batch_tracked(files_info, target_base_path, api_key, task_id, batch_size=50):
    """AIç”Ÿæˆåˆ†ç±»æ–¹æ¡ˆ - åˆ†æ‰¹å¤„ç†ç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡è¿½è¸ªå’Œæ—¶é—´ç»Ÿè®¡"""
    
    # æ‰«æç°æœ‰ç›®å½•ç»“æ„
    existing_structure = scan_target_directory_structure(target_base_path)
    
    # å¤„ç†ç©ºç›®å½•çš„æƒ…å†µ
    if not existing_structure:
        standard_para_dirs = ['01-Projects', '02-Areas', '03-Resources', '04-Archives']
        for dir_name in standard_para_dirs:
            dir_path = os.path.join(target_base_path, dir_name)
            os.makedirs(dir_path, exist_ok=True)
        existing_structure = scan_target_directory_structure(target_base_path)
    
    # åˆ†æ‰¹å¤„ç†
    total_files = len(files_info)
    total_batches = (total_files + batch_size - 1) // batch_size
    
    all_mapping_tables = []
    merged_directory_structure = {}  # ç›´æ¥ä½¿ç”¨å­—å…¸è€Œä¸æ˜¯åˆ—è¡¨
    all_discussion_points = []
    successful_batches = 0
    
    # æ—¶é—´ç»Ÿè®¡
    import time
    batch_times = []  # è®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„å¤„ç†æ—¶é—´
    total_start_time = time.time()
    
    for batch_num in range(total_batches):
        batch_start_time = time.time()
        
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, total_files)
        batch_files = files_info[start_idx:end_idx]
        
        # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
        if batch_times:
            avg_batch_time = sum(batch_times) / len(batch_times)
            remaining_batches = total_batches - batch_num
            estimated_remaining_time = avg_batch_time * remaining_batches
            
            # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
            if estimated_remaining_time < 60:
                time_str = f"{int(estimated_remaining_time)}ç§’"
            elif estimated_remaining_time < 3600:
                minutes = int(estimated_remaining_time // 60)
                seconds = int(estimated_remaining_time % 60)
                time_str = f"{minutes}åˆ†{seconds}ç§’"
            else:
                hours = int(estimated_remaining_time // 3600)
                minutes = int((estimated_remaining_time % 3600) // 60)
                time_str = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"
        else:
            # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œç»™å‡ºç»éªŒé¢„ä¼°ï¼šæ¯æ‰¹æ¬¡çº¦1-3åˆ†é’Ÿ
            estimated_per_batch = 90  # 90ç§’çš„ç»éªŒå€¼
            estimated_remaining_time = estimated_per_batch * (total_batches - batch_num)
            time_str = f"çº¦{int(estimated_remaining_time/60)}åˆ†é’Ÿ"
        
        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        batch_progress = 70 + int((batch_num / total_batches) * 20)  # AIåˆ†æé˜¶æ®µå 70-90%
        analysis_tasks[task_id]['stage_progress'] = batch_progress
        analysis_tasks[task_id]['message'] = f'AIæ­£åœ¨åˆ†æç¬¬ {batch_num + 1}/{total_batches} æ‰¹æ¬¡ ({len(batch_files)} ä¸ªæ–‡ä»¶)ï¼Œé¢„è®¡è¿˜éœ€ {time_str}...'
        analysis_tasks[task_id]['estimated_remaining_time'] = estimated_remaining_time if batch_times else None
        analysis_tasks[task_id]['average_batch_time'] = sum(batch_times) / len(batch_times) if batch_times else None
        
        if batch_times:
        
        try:
            # è°ƒç”¨åŸæœ‰çš„AIåˆ†æå‡½æ•°å¤„ç†è¿™ä¸€æ‰¹æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            batch_result = None
            max_retries = 2
            
            for retry_count in range(max_retries + 1):
                try:
                    batch_result = generate_classification_plan_with_ai(batch_files, target_base_path, api_key)
                    if batch_result:
                        break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                    else:
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {batch_num + 1} æ‰¹æ¬¡ç¬¬ {retry_count + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if retry_count < max_retries:
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    else:
                        print(f"âŒ ç¬¬ {batch_num + 1} æ‰¹æ¬¡æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
            
            # è®°å½•æ‰¹æ¬¡å¤„ç†æ—¶é—´
            batch_end_time = time.time()
            batch_duration = batch_end_time - batch_start_time
            batch_times.append(batch_duration)
            
            if batch_result:
                successful_batches += 1
                
                # æ”¶é›†ç»“æœ
                all_mapping_tables.extend(batch_result.get('mapping_table', []))
                
                # åˆå¹¶ç›®å½•ç»“æ„ï¼ˆæ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼‰
                batch_structure = batch_result.get('directory_structure', {})
                for top_dir, sub_structure in batch_structure.items():
                
                
                for top_dir, sub_structure in batch_structure.items():
                    if top_dir not in merged_directory_structure:
                        merged_directory_structure[top_dir] = sub_structure
                    else:
                        merge_directory_structures(merged_directory_structure[top_dir], sub_structure)
                
                for top_dir in merged_directory_structure:
                    if isinstance(merged_directory_structure[top_dir], dict):
                    else:
                
                # æ”¶é›†è®¨è®ºç‚¹
                all_discussion_points.extend(batch_result.get('discussion_points', []))
                
                # æ›´æ–°æˆåŠŸè¿›åº¦ï¼ŒåŒ…å«æ—¶é—´ä¿¡æ¯
                analysis_tasks[task_id]['message'] = f'å·²å®Œæˆ {batch_num + 1}/{total_batches} æ‰¹æ¬¡ï¼ŒæˆåŠŸåˆ†ç±» {len(all_mapping_tables)} ä¸ªæ–‡ä»¶'
            else:
                print(f"âŒ ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œè€—æ—¶ {batch_duration:.1f}ç§’")
                # æ›´æ–°é”™è¯¯ä¿¡æ¯ä½†ç»§ç»­å¤„ç†
                analysis_tasks[task_id]['message'] = f'ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¤±è´¥ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ‰¹æ¬¡...'
                
        except Exception as e:
            batch_end_time = time.time()
            batch_duration = batch_end_time - batch_start_time
            batch_times.append(batch_duration)  # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•æ—¶é—´ï¼Œç”¨äºé¢„ä¼°
            
            print(f"ğŸ’¥ ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¤„ç†å¼‚å¸¸ï¼Œè€—æ—¶ {batch_duration:.1f}ç§’: {e}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            analysis_tasks[task_id]['message'] = f'ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¼‚å¸¸ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ‰¹æ¬¡...'
            continue
    
    # æ€»å¤„ç†æ—¶é—´ç»Ÿè®¡
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    if all_mapping_tables:
        # ä½¿ç”¨åˆå¹¶åçš„ç›®å½•ç»“æ„ï¼Œä¸é‡æ–°æ„å»º
        for top_dir in merged_directory_structure:
        
        final_result = {
            'mapping_table': all_mapping_tables,
            'directory_structure': merged_directory_structure,  # ç›´æ¥ä½¿ç”¨åˆå¹¶åçš„ç»“æ„
            'discussion_points': all_discussion_points
        }
        
        success_rate = successful_batches / total_batches * 100
        avg_time_per_batch = sum(batch_times) / len(batch_times) if batch_times else 0
        
        
        # æ›´æ–°æœ€ç»ˆçŠ¶æ€ï¼ŒåŒ…å«æ—¶é—´ç»Ÿè®¡
        analysis_tasks[task_id]['message'] = f'åˆ†æ‰¹åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†ç±» {len(all_mapping_tables)} ä¸ªæ–‡ä»¶ (æˆåŠŸç‡ {success_rate:.1f}%ï¼Œæ€»è€—æ—¶ {total_duration/60:.1f}åˆ†é’Ÿ)'
        analysis_tasks[task_id]['total_duration'] = total_duration
        analysis_tasks[task_id]['average_batch_time'] = avg_time_per_batch
        analysis_tasks[task_id]['estimated_remaining_time'] = 0  # å·²å®Œæˆ
        
        return final_result
    else:
        print("âŒ æ‰€æœ‰æ‰¹æ¬¡éƒ½å¤„ç†å¤±è´¥")
        analysis_tasks[task_id]['message'] = 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyå’Œç½‘ç»œè¿æ¥'
        return None

def merge_directory_structures(target, source):
    """åˆå¹¶ä¸¤ä¸ªç›®å½•ç»“æ„ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
    
    for key, value in source.items():
        if key in target:
            if isinstance(target[key], dict) and isinstance(value, dict):
                # ä¸¤è¾¹éƒ½æ˜¯å­—å…¸ï¼Œé€’å½’åˆå¹¶
                merge_directory_structures(target[key], value)
            elif isinstance(target[key], list) and isinstance(value, list):
                # ä¸¤è¾¹éƒ½æ˜¯åˆ—è¡¨ï¼Œåˆå¹¶å¹¶å»é‡
                target[key] = list(set(target[key] + value))
            elif isinstance(target[key], dict) and isinstance(value, list):
                # ç›®æ ‡æ˜¯å­—å…¸ï¼Œæºæ˜¯åˆ—è¡¨ï¼Œå°†åˆ—è¡¨åŠ åˆ°å­—å…¸ä¸­
                if '__files__' not in target[key]:
                    target[key]['__files__'] = []
                target[key]['__files__'].extend(value)
            elif isinstance(target[key], list) and isinstance(value, dict):
                # ç›®æ ‡æ˜¯åˆ—è¡¨ï¼Œæºæ˜¯å­—å…¸ï¼Œå°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ä¸­çš„æ–‡ä»¶
                new_dict = {'__files__': target[key]}
                new_dict.update(value)
                target[key] = new_dict
            else:
                # å…¶ä»–æƒ…å†µï¼Œæºè¦†ç›–ç›®æ ‡
                target[key] = value
        else:
            target[key] = value
    
    return target

def add_to_directory_structure(structure, file_path):
    """æ ¹æ®æ–‡ä»¶è·¯å¾„æ„å»ºç›®å½•ç»“æ„"""
    # ç§»é™¤æ–‡ä»¶åï¼Œåªä¿ç•™ç›®å½•è·¯å¾„
    parts = file_path.split('/')[:-1]  # æ’é™¤æ–‡ä»¶å
    
    current = structure
    for part in parts:
        if part and part.strip():  # è·³è¿‡ç©ºå­—ç¬¦ä¸²
            if part not in current:
                current[part] = {}
            current = current[part]

def analyze_files_async(task_id, source_path, target_path, api_key):
    """å¼‚æ­¥åˆ†ææ–‡ä»¶ - æ–°çš„æ•´ä½“åˆ†ææµç¨‹"""
    try:
        
        # é˜¶æ®µ1ï¼šæ‰«ææ–‡ä»¶
        analysis_tasks[task_id]['status'] = 'scanning'
        analysis_tasks[task_id]['message'] = 'æ­£åœ¨æ‰«ææ–‡ä»¶å¤¹...'
        analysis_tasks[task_id]['stage'] = 'scanning'
        analysis_tasks[task_id]['stage_progress'] = 10
        
        files = scan_files(source_path)
        total_files = len(files)
        
        analysis_tasks[task_id]['total_files'] = total_files
        analysis_tasks[task_id]['found_files'] = total_files
        
        # é˜¶æ®µ2ï¼šæ”¶é›†æ–‡ä»¶ä¿¡æ¯
        analysis_tasks[task_id]['status'] = 'collecting'
        analysis_tasks[task_id]['message'] = 'æ­£åœ¨æ”¶é›†æ–‡ä»¶ä¿¡æ¯...'
        analysis_tasks[task_id]['stage'] = 'collecting'
        analysis_tasks[task_id]['stage_progress'] = 30
        analysis_tasks[task_id]['processed_files'] = 0
        
        files_info = []
        for i, file_path in enumerate(files):
            analysis_tasks[task_id]['processed_files'] = i + 1
            analysis_tasks[task_id]['current_file'] = os.path.basename(file_path)
            analysis_tasks[task_id]['stage_progress'] = 30 + int((i + 1) / total_files * 30)  # 30-60%
            
            if (i + 1) % 10 == 0 or i == 0:  # æ¯10ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡è¿›åº¦
            
            try:
                file_info = get_file_info(file_path)
                if file_info:
                    content_preview = read_file_content(file_path)
                    file_info['content_preview'] = content_preview
                    files_info.append(file_info)
            except Exception as e:
                print(f"âŒ [ä»»åŠ¡ {task_id}] æ”¶é›†æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
                continue
        
        
        # é˜¶æ®µ3ï¼šAIæ™ºèƒ½åˆ†æï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰
        analysis_tasks[task_id]['status'] = 'ai_analyzing'
        analysis_tasks[task_id]['message'] = 'AIæ­£åœ¨åˆ†ææ‰€æœ‰æ–‡ä»¶å¹¶ç”Ÿæˆåˆ†ç±»æ–¹æ¡ˆ...'
        analysis_tasks[task_id]['stage'] = 'ai_analyzing'
        analysis_tasks[task_id]['stage_progress'] = 70
        analysis_tasks[task_id]['current_file'] = ''  # æ¸…ç©ºå½“å‰æ–‡ä»¶ï¼Œå› ä¸ºAIæ˜¯æ‰¹é‡å¤„ç†
        
        if not files_info:
            analysis_tasks[task_id]['status'] = 'completed'
            analysis_tasks[task_id]['message'] = 'æœªæ‰¾åˆ°å¯åˆ†æçš„æ–‡ä»¶'
            analysis_tasks[task_id]['results'] = {}
            analysis_tasks[task_id]['stage_progress'] = 100
            return

        
        # æ ¹æ®æ–‡ä»¶æ•°é‡å†³å®šæ˜¯å¦åˆ†æ‰¹å¤„ç†
        def get_optimal_batch_size(total_files):
            """æ ¹æ®æ–‡ä»¶æ•°é‡æ™ºèƒ½ç¡®å®šæœ€ä½³æ‰¹æ¬¡å¤§å°"""
            if total_files <= 50:
                return total_files  # å°äº50ä¸ªæ–‡ä»¶ä¸åˆ†æ‰¹
            elif total_files <= 200:
                return 40  # ä¸­ç­‰æ•°é‡ä½¿ç”¨40
            elif total_files <= 500:
                return 30  # è¾ƒå¤§æ•°é‡ä½¿ç”¨30
            else:
                return 25  # å¤§é‡æ–‡ä»¶ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡ï¼Œæ›´ç¨³å®š
        
        batch_size = get_optimal_batch_size(len(files_info))
        use_batch_processing = len(files_info) > 50  # è¶…è¿‡50ä¸ªæ–‡ä»¶æ‰åˆ†æ‰¹
        
        if use_batch_processing:
            total_batches = (len(files_info) + batch_size - 1) // batch_size
            analysis_tasks[task_id]['message'] = f'AIæ­£åœ¨åˆ†æ‰¹åˆ†æ {len(files_info)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_batches} ä¸ªæ‰¹æ¬¡...'
            classification_plan = generate_classification_plan_with_ai_batch_tracked(files_info, target_path, api_key, task_id, batch_size)
        else:
            analysis_tasks[task_id]['message'] = f'AIæ­£åœ¨åˆ†æ {len(files_info)} ä¸ªæ–‡ä»¶...'
            classification_plan = generate_classification_plan_with_ai(files_info, target_path, api_key)
        
        # é˜¶æ®µ4ï¼šå¤„ç†ç»“æœ
        analysis_tasks[task_id]['status'] = 'processing'
        analysis_tasks[task_id]['message'] = 'æ­£åœ¨å¤„ç†åˆ†æç»“æœ...'
        analysis_tasks[task_id]['stage'] = 'processing'
        analysis_tasks[task_id]['stage_progress'] = 90
        
        if classification_plan:
            # å°†åŸå§‹çš„ source_path æ·»åŠ åˆ° mapping_table ä¸­ï¼Œä»¥å¤‡è¿ç§»ä½¿ç”¨
            source_path_map = {info['name']: info['path'] for info in files_info}
            for item in classification_plan.get('mapping_table', []):
                item['source_path'] = source_path_map.get(item['filename'])

            analysis_tasks[task_id]['status'] = 'completed'
            analysis_tasks[task_id]['message'] = 'åˆ†æå®Œæˆ'
            analysis_tasks[task_id]['stage'] = 'completed'
            analysis_tasks[task_id]['stage_progress'] = 100
            analysis_tasks[task_id]['results'] = classification_plan
        else:
            print(f"âŒ [ä»»åŠ¡ {task_id}] AIåˆ†ç±»æ–¹æ¡ˆç”Ÿæˆå¤±è´¥")
            analysis_tasks[task_id]['status'] = 'error'
            analysis_tasks[task_id]['message'] = 'æ™ºèƒ½åˆ†ç±»æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æ—¥å¿—ã€‚'
            analysis_tasks[task_id]['stage'] = 'error'

    except Exception as e:
        print(f"ğŸ’¥ [ä»»åŠ¡ {task_id}] åˆ†æè¿‡ç¨‹å¼‚å¸¸: {e}")
        import traceback
        print(f"ğŸ“‹ [ä»»åŠ¡ {task_id}] å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        analysis_tasks[task_id]['status'] = 'error'
        analysis_tasks[task_id]['message'] = f'åˆ†æå¤±è´¥: {str(e)}'
        analysis_tasks[task_id]['stage'] = 'error'

def scan_files(source_path):
    """æ‰«ææ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    files = []
    try:
        for root, dirs, filenames in os.walk(source_path):
            # æ’é™¤å¸¸è§çš„æ— éœ€æ•´ç†çš„ç›®å½•
            dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', '__pycache__', 'venv']]
            for filename in filenames:
                # æ’é™¤å¸¸è§çš„æ— éœ€æ•´ç†çš„æ–‡ä»¶
                if filename in ['.DS_Store']:
                    continue
                file_path = os.path.join(root, filename)
                files.append(file_path)
    except Exception as e:
        print(f"æ‰«ææ–‡ä»¶å¤±è´¥: {e}")
    
    return files

@classifier_bp.route('/test-api-key', methods=['POST'])
def test_api_key():
    """æµ‹è¯•API Keyæ˜¯å¦æœ‰æ•ˆ"""
    try:
        data = request.get_json()
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({'error': 'ç¼ºå°‘ API Key'}), 400

        
        # ä½¿ç”¨è±†åŒ…SDKæµ‹è¯•
        client = Ark(api_key=api_key)
        
        completion = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'"}],
            temperature=0.1,
            max_tokens=50
        )
        
        ai_response = completion.choices[0].message.content
        
        return jsonify({
            'success': True,
            'message': 'API Key éªŒè¯æˆåŠŸ',
            'model': DEFAULT_MODEL,
            'api_provider': 'è±†åŒ…-ç«å±±å¼•æ“',
            'test_response': ai_response
        })
            
    except Exception as e:
        error_msg = str(e)
        print(f"è±†åŒ…API Keyæµ‹è¯•å¼‚å¸¸: {error_msg}")
        
        # åˆ¤æ–­é”™è¯¯ç±»å‹
        if 'authentication' in error_msg.lower() or 'api key' in error_msg.lower() or 'unauthorized' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': 'API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ',
                'detail': error_msg
            }), 401
        elif 'permission' in error_msg.lower() or 'forbidden' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': 'æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥API Keyæƒé™æˆ–è´¦æˆ·ä½™é¢',
                'detail': error_msg
            }), 403
        elif 'rate limit' in error_msg.lower() or 'too many' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•',
                'detail': error_msg
            }), 429
        elif 'timeout' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': 'APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥',
                'detail': error_msg
            }), 408
        elif 'connection' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': 'æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥',
                'detail': error_msg
            }), 503
        else:
            return jsonify({
                'success': False,
                'error': f'æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_msg}',
                'detail': error_msg
            }), 500

@classifier_bp.route('/analyze', methods=['POST'])
def analyze_files():
    """å¼€å§‹æ–‡ä»¶åˆ†æ"""
    try:
        data = request.get_json()
        source_path = data.get('source_path')
        target_path = data.get('target_path')
        api_key = data.get('api_key') # è·å–API Key
        
        if not source_path or not target_path:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        if not api_key:
            return jsonify({'error': 'ç¼ºå°‘ API Key'}), 400

        if not os.path.exists(source_path):
            return jsonify({'error': 'æºæ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 400
        
        task_id = str(uuid.uuid4())
        
        analysis_tasks[task_id] = {
            'status': 'started',
            'message': 'å¼€å§‹åˆ†æ...',
            'total_files': 0,
            'processed_files': 0,
            'current_file': '',
            'results': {},
            'stage': 'started',
            'stage_progress': 0,
            'found_files': 0,
            'created_at': datetime.now().isoformat()
        }
        
        thread = threading.Thread(
            target=analyze_files_async,
            args=(task_id, source_path, target_path, api_key) # å°†API Keyä¼ é€’ç»™çº¿ç¨‹
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({'task_id': task_id, 'message': 'åˆ†æå·²å¼€å§‹'})
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@classifier_bp.route('/classification/<task_id>', methods=['GET'])
def get_classification_status(task_id):
    """è·å–åˆ†ç±»çŠ¶æ€å’Œç»“æœ"""
    if task_id not in analysis_tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = analysis_tasks[task_id]
    
    response = {
        'task_id': task_id,
        'status': task['status'],
        'message': task['message'],
        'total_files': task['total_files'],
        'processed_files': task['processed_files'],
        'current_file': task['current_file'],
        'results': task.get('results', {}),
        'stage': task.get('stage', task['status']),
        'stage_progress': task.get('stage_progress', 0),
        'found_files': task.get('found_files', task['total_files']),
        # æ·»åŠ æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
        'estimated_remaining_time': task.get('estimated_remaining_time'),
        'average_batch_time': task.get('average_batch_time'),
        'total_duration': task.get('total_duration')
    }
    
    return jsonify(response)

@classifier_bp.route('/directory-structure/<task_id>', methods=['GET'])
def get_directory_structure(task_id):
    """è·å–ç”Ÿæˆçš„ç›®å½•ç»“æ„"""
    if task_id not in analysis_tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = analysis_tasks[task_id]
    
    return jsonify({
        'task_id': task_id,
        'directory_structure': task.get('directory_structure', {}),
        'status': task['status']
    })

@classifier_bp.route('/migrate', methods=['POST'])
def migrate_files():
    """æ‰§è¡Œæ–‡ä»¶è¿ç§»"""
    try:
        data = request.get_json()
        classifications = data.get('classifications', [])
        
        for i, item in enumerate(classifications[:3]):  # åªæ‰“å°å‰3ä¸ªä½œä¸ºç¤ºä¾‹
        if len(classifications) > 3:
        
        if not classifications:
            return jsonify({'error': 'æ²¡æœ‰è¦è¿ç§»çš„æ–‡ä»¶'}), 400
        
        results = {
            'success': [],
            'failed': [],
            'skipped': []
        }
        
        for i, item in enumerate(classifications):
            source_path = item.get('source_path')
            target_path = item.get('target_path')
            filename = os.path.basename(source_path) if source_path else 'unknown'
            
            
            try:
                # éªŒè¯è·¯å¾„
                if not source_path or not target_path:
                    error_msg = f"è·¯å¾„ä¿¡æ¯ä¸å®Œæ•´: source={source_path}, target={target_path}"
                    print(f"âŒ {error_msg}")
                    results['failed'].append({
                        'source': source_path,
                        'target': target_path,
                        'error': error_msg
                    })
                    continue
                
                # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(source_path):
                    error_msg = f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_path}"
                    print(f"âŒ {error_msg}")
                    results['failed'].append({
                        'source': source_path,
                        'target': target_path,
                        'error': 'æºæ–‡ä»¶ä¸å­˜åœ¨'
                    })
                    continue
                
                # åˆ›å»ºç›®æ ‡ç›®å½•
                target_dir = os.path.dirname(target_path)
                os.makedirs(target_dir, exist_ok=True)
                
                # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if os.path.exists(target_path):
                    results['skipped'].append({
                        'source': source_path,
                        'target': target_path,
                        'reason': 'ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨'
                    })
                    continue
                
                # ç§»åŠ¨æ–‡ä»¶
                os.rename(source_path, target_path)
                
                results['success'].append({
                    'source': source_path,
                    'target': target_path
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"ğŸ’¥ æ–‡ä»¶è¿ç§»å¤±è´¥ {filename}: {error_msg}")
                results['failed'].append({
                    'source': source_path,
                    'target': target_path,
                    'error': error_msg
                })
        
        # æ‰“å°è¿ç§»æ‘˜è¦
        print(f"  âŒ å¤±è´¥: {len(results['failed'])} ä¸ªæ–‡ä»¶")
        
        return jsonify({
            'message': 'è¿ç§»å®Œæˆ',
            'results': results,
            'summary': {
                'total': len(classifications),
                'success': len(results['success']),
                'failed': len(results['failed']),
                'skipped': len(results['skipped'])
            }
        })
        
    except Exception as e:
        print(f"ğŸ’¥ è¿ç§»è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        print(f"ğŸ“‹ å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        return jsonify({'error': str(e)}), 500

